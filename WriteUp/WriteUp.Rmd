---
title: 'Data Science Methods for Economics and Finance 871 Final Project: Predicting
  Chess Outcomes'
documentclass: elsarticle
Thesis_FP: no
output:
  pdf_document:
    keep_tex: yes
    template: Tex/TexDefault.txt
    fig_width: 3.5
    fig_height: 3.5
  html_document:
    df_print: paged
Author1: 'Wesley Williams^[Special Thanks to Ruan Geldenhuys for his help.]'
Ref1: Stellenbosch University, South Africa
Email1: 21691126\@sun.ac.za
BottomRFooter: \footnotesize Page \thepage
addtoprule: yes
addfootrule: yes
margin: 2.3
bottom: 2
top: 2.5
HardSet_layout: yes
linenumbers: no
bibliography: Tex/ref.bib
csl: "Tex/harvard-stellenbosch-university.csl"
RemovePreprintSubmittedTo: yes
Journal: Journal of Finance
toc: yes
numbersections: yes
fontsize: 12pt
linestretch: 1.2
link-citations: yes
AddTitle: yes
abstract: |
 Chess is one of the most complicated games in the world. Humans on the other hand are not so complicated and we are the ones playing the game. This made me ask the question: "Is it possible to predict a game of chess based on just on data on the players before the game has started and just the openings used?" I employ an extreme gradient boosting model to answer this question and find that I can only predict the outcome with just over 60% accuracy, which is double the chances of just guessing.  You can find my code on my Github page: https://github.com/wjwilliams/MLPROJ/tree/main/WriteUp
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.height = 5,
	fig.pos = "H",
	fig.width = 6,
	message = FALSE,
	warning = FALSE
)
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(caret)
library(xtable)
library(rsample)
library(xgboost)
library(DiagrammeR)
library(Ckmeans.1d.dp)
library(glue)

```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->
```{r IMPORTDATA&cleaning}
master_df <- read.csv("data/games.csv")

#drop unuseful collumns
master_df <- subset(master_df, select = c(-white_id, -black_id, -id, -created_at, -last_move_at))

#Drop any games with less than 11 moves so we can use the first 5 moves
master_df <- master_df %>%
  filter(str_count(moves, "\\S+") >= 11)

#Create a difference variables of the difference in ratings with respect to white such that if white has an elo of 400 and black 500 the difference will be -100
master_df$ratingdiff <- master_df$white_rating - master_df$black_rating

#Extracting the first five moves for White and black
##This removes everything after the first space which leaves the first move in chess notation eg e4 represents the first move for white movinf the pawn to the e4 square
master_df <- master_df %>%
  separate(moves, into = c("whitemove1", "blackmove1", "whitemove2", "blackmove2", "whitemove3","blackmove3", "whitemove4","blackmove4", "whitemove5", "blackmove5"), sep = " ", extra = "drop", fill = "right") 
  
```
# Introduction \label{Introduction}
Chess, often considered one of the most popular games globally, has experienced a surge in popularity in recent years, partly fueled by its portrayal in popular media such as the acclaimed TV show "Queen's Gambit" and the movie "Pawn." These portrayals, although fictionalized to varying degrees, shed light on the remarkable life and achievements of renowned chess player Bobby Fischer. The story of Bobby Fischer is both inspirational and incredibly sad. During the Cold War, the United States (US) and the Soviet Union (USSR) engaged in a fierce competition to assert their global dominance. This rivalry extended beyond conventional arenas like the space race and nuclear arms race, encompassing intellectual pursuits such as chess. Chess was revered as the game of the intellectual elite, and both nations sought to establish themselves as the preeminent force in this domain, symbolizing their intellectual superiority. The USSR dominated until Fischer's victory against world champion Boris Spassky, symbolizing a small triumph for the US in the Cold War context. This illustrates that chess extends beyond a mere board game, carrying significant cultural and geopolitical implications. While the stakes are not as high in the modern era, chess is still seen as the epitome of intellect.

Chess can be broken down into three stages of the game: the opening, the middlegame and the endgame. The opening represents the different strategies of getting all of your pieces into the most optimal positions on the board to both attack your opponents pieces and defend your own. This is the key part of a chess game that will be investigated in the paper. I want to determine if the outcome of a chess game can be predicted using only the information available about the game and players before the game begins and the openings employed by the players. The use of machine learning is crucial in this analysis due to the complexity of the game. I use an extreme gradient boosting model (XGBOOST) with three potential outcomes (white winning, black winning or a draw). The outcome is not binary therefore I cannot use a normal ordinary least squares as comparison and so I use the probability of guessing as the baseline. I also subset the data by ELO rating to assess whether it is easier to predict weaker or stronger players. I find that the opening is slightly more important for weaker players but the model is not as accurate as when using the entire sample. The rest of the paper is structured as follows, in section 2 I describe the data section 3 explores and visualizes the data, section 4 explains the methodology, section 5 presents the results and section 6 concludes.

# Data
The data was obtained from Kaggle and it includes over 20,000 games played on the online chess platform "lichess". The dataset includes 16 variables but there are only seven variables of interest: 1) the winner given as white, black or draw; 2) time and increment code which details the time control as the base time and the additional time per move; 3) white's ELO rating; 4) black's ELO rating; 5) moves which are all the moves in the game given in chess notation; 6) opening eco which is a code that represents the opening that was played and 7) opening ply which represents the number of moves played in the opening that corresponds to chess theory. An additional variable of rating difference was engineered from the perspective of white, which is just white's rating minus black's rating. 

## Factor engineering
Some of the variables need to be wrangled to be used in the model. All of the numerical variables are sufficient for the model but the character variables need to be engineered to be included. Firstly, I am only interested in the first five moves of the game, I therefore expand the moves variable so that I have a variable for each of the first five moves played by each player and disregard the rest. The moves are then converted from chess notation to a unique numerical factor for each piece moving to each co-ordinate on the board. Secondly, I separate the increment variable into the base time and the increment for each move and ensure both are numerical factors. Lastly, I assign a unique numerical value to each of the unique openeing codes to ensure comparability between the training as testings samples\footnote{I did attempt one-hot encoding but I had issues with the differences in lengths between the training and testing samples}. All the variables are therefore sufficiently engineered to be used in the model.

# Exploratory Data Analysis
In this section I attempt to explore and visualize the data to gain insights into the patterns that emerge with respect all the features and the the target. 

```{r commove1, fig.align='center', fig.cap="Most Common Opening Moves by Colour\\label{Figure1}", fig.ext='png', fig.height=4, fig.width= 5, warning=FALSE}
#First lets just see the ratio of game outcomes by time controls, rating, rating difference, and type of win (timeout, checkmate and resignation)
library(dplyr)
whitemovepop <- master_df %>%
  count(whitemove1) %>%
  top_n(10) %>%
  arrange(desc(n))

blackmovepop <- master_df %>%
  count(blackmove1) %>%
  top_n(10) %>%
  arrange(desc(n))

# Combine the most common moves for white and black
movepop <- bind_rows(
 mutate(whitemovepop, side = "White", move = whitemove1),
  mutate(blackmovepop, side = "Black", move = blackmove1)
)

# Plot the bar plot

plot1 <- ggplot(movepop, aes(x = fct_inorder(move), y = n, fill = side)) +
  geom_bar(stat = "identity") +
  labs(x = "Move 1", y = "Frequency", title = "10 Most Common First Move") +
  scale_fill_manual(values = c("White" = "gray", "Black" = "black")) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())

plot1
```

```{r firstmovepropwins, fig.align='center', fig.cap="Proportions of Outcomes by White First Move\\label{Figure2}", fig.ext='png', fig.height=3.5, fig.width=4, warning=FALSE}
#Now to see the proportions of wins relative to the most popular openings
outcome_proportions <- master_df %>%
  group_by(whitemove1) %>%
  summarize(
    white_wins = sum(winner == "white"),
    black_wins = sum(winner == "black"),
    draws = sum(winner == "draw"),
    total_games = n()
  ) %>%
  mutate(
    white_win_prop = white_wins / total_games,
    black_win_prop = black_wins / total_games,
    draw_prop = draws / total_games
  )


filtered_proportions <- outcome_proportions %>%
  filter(whitemove1 %in% c("e4", "d4", "Nf3", "c4", "e3"))

filtered_proportions_long <- tidyr::pivot_longer(filtered_proportions,
                                                 cols = c(white_win_prop, black_win_prop, draw_prop),
                                                 names_to = "outcome",
                                                 values_to = "proportion")


outcomeplot1 <- ggplot(filtered_proportions_long) +
  geom_col(aes(x = whitemove1, y = proportion, fill = outcome), position = "stack") +
  labs(x = "First Move", y = "Win rate", title = "Win Percentage by Colour for First Moves") +
  scale_fill_manual(values = c("white_win_prop" = "gray", "black_win_prop" = "black", "draw_prop" = "blue"),
                    labels = c("Black Wins", "Draws", "White Wins")) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_discrete(limits = c("e4", "d4", "Nf3", "c4", "e3")) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())

outcomeplot1
```

Figure 3.1 shows the most common first move for each colour. It is no surprise that central pawn moves are the most common as controlling the center of the board is instrumental in the opening phase of a chess game. Figure 3.2 shows the outcome proportions for white's first move and it shows that if white claims the center they gain an advantage and black needs to respond. The move e3 instead allows black to claim the center and white loses its first mover advantage. This highlights that mistakes early on in the openings have consequences that last throughout the entire game.


```{r Descript3wins, fig.align='center', fig.cap="Outcomes by Time Controls\\label{Figure3}", fig.ext='png', fig.height=4, fig.width=5, warning=FALSE}
#Lets look at the differences in time controls
#first lets look at if there is a change in winning depending on if there is added time per move
incrementdf <- master_df %>% 
  separate(increment_code, into = c("base", "increment"), sep = "\\+") %>% 
    mutate(incr = ifelse(increment != 0, "no", "yes")) %>% 
    select(winner, base, increment, incr) %>% 
    group_by(base, incr) %>% 
    summarize(
    white_wins = sum(winner == "white"),
    black_wins = sum(winner == "black"),
    draws = sum(winner == "draw"),
    total_games = n()
  ) %>%
  mutate(
    white_win_prop = white_wins / total_games,
    black_win_prop = black_wins / total_games,
    draw_prop = draws / total_games
  ) %>% 
    ungroup() %>% 
    filter(total_games > 200, base != 5, base != 7) %>% 
    select(base, incr, white_win_prop, black_win_prop, draw_prop) %>% 
    gather(Score, Value, -base, -incr)

#now lets plot a facetwrapped plot to see if there are differences between increments being present or not

incrplot1 <- ggplot(incrementdf, aes(x = base, y = Value, fill = Score)) +
  geom_col(aes(x = base, y = Value, fill = Score), position = "stack") +
    facet_wrap(~incr, scales = "fixed", nrow = 2)+
  labs(x = "Time Control", y = "Win rate", title = "Win Percentage by Time Control", subtitle = "Grouped by increment or not") +
  scale_fill_manual(values = c("white_win_prop" = "gray", "black_win_prop" = "black", "draw_prop" = "blue"),
                    labels = c("Black Wins", "Draws", "White Wins")) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
  theme(legend.position = "bottom")

incrplot1
```
Figure 3.3 shows the differences in outcome according to different time controls. It presents the top 5 time controls that are played with and without time increments. There are no significant differences in wins between whether a game has time increments or not. The only real difference is that a draw is more likely with larger time controls and no time increments.
```{r Desc4commonopen, fig.align='center', fig.cap="Most Popular Openings\\label{Figure4}", fig.ext='png', fig.height=3.5, fig.width=4, warning=FALSE}
#Now lets look at the most common openings
openings <- master_df %>% 
    select(opening_name) %>% 
    separate(opening_name, into = c("opening", "variation"), sep = ": ") %>% 
    group_by(opening) %>% 
    count(opening) %>%
    ungroup() %>% 
    arrange(-n) %>% 
    top_n(10) %>% 
    select(n, opening)

#now we have the top 10 openings lets plot them
pop_openings1<- openings %>% 
    ggplot()+
    geom_col(aes(x=opening,y=n))+
                 coord_flip()+
                 theme_classic()+
    labs(title= "Most Common Openings", y = "Count")
pop_openings1
```

```{r openplot1, fig.align='center', fig.cap="Outcome by Popular Openings\\label{Figure5}", fig.ext='png', fig.height=4, fig.width=5, warning=FALSE}
#Now that we have the most popular openings are lets look at the proportion of winners and losers are
winner_openings <- master_df %>% 
  separate(opening_name, into = c("opening", "variation"), sep = ": ") %>% 
  select(opening, winner) %>% 
  filter(opening %in% c("Sicilian Defense", "French Defense", "Queen's Pawn Game", "Italian Game", "King's Pawn Game", "Ruy Lopez", "English Opening", "Scandinavian Defense", "Caro-Kann Defense", "Scotch Game")) %>% 
  group_by(opening) %>% 
  summarize(
    white_wins = sum(winner == "white"),
    black_wins = sum(winner == "black"),
    draws = sum(winner == "draw"),
    total_games = n()
  ) %>%
  mutate(
    white_win_prop = white_wins / total_games,
    black_win_prop = black_wins / total_games,
    draw_prop = draws / total_games
  ) %>% 
    select(opening, white_win_prop, black_win_prop, draw_prop) %>% 
  gather(Score, Value, -opening)

opening_plot1 <- winner_openings %>% 
  ggplot() +
  geom_col(aes(x = opening, y = Value, fill = Score), position = "stack") +
  labs(x = "Opening", y = "Win Proportion") +
  scale_fill_manual(values = c("white_win_prop" = "gray", "black_win_prop" = "black", "draw_prop" = "blue"),
                    labels = c("Black Wins", "Draws", "White Wins")) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal() +
    coord_flip()+
  theme(legend.position = "bottom", legend.title = element_blank())+
    theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))

opening_plot1

    

```
Figures 3.4 and 3.5 show the most played openings and the win proportions of those openings respectively. The opening names were given in the dataset with their variations (e.g. Queen's Gambit: Declined) where I only want the base opening name. I therefore separated the names and then dropped the variation. The Sicilian defense being the most popular is interesting as it is initiated by black as a response to white's first move of e4 who then may need to change their strategy in the opening. The opening is usually initiated by white and black has to adjust their strategy. The power of the Sicilian defense is show in figure 3.5 with the largest proportion of black winning out of all the common openings at 50%. The reason for this is that it partially eliminates white's first mover advantage as they have to respond in a way that may not have been their plan. These figures not only highlight the importance of the opening as one can gain a significant advantage that can carry through the game but also show that some openings favour a colour.
```{r ratingoutcomescatter, fig.align='center', fig.cap="Outcome and Ratings\\label{Figure6}", fig.ext='png', fig.height=4, fig.width=5, warning=FALSE}
winner_rating<- master_df %>% 
ggplot()+
    geom_point(aes(x=white_rating,y=black_rating,color=winner), alpha=0.5)+
      geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black")+
    labs(title = "Outcome by Each Colour's Ratings", x = "Black Rating", y = "White Rating")

winner_rating
```

Figure 3.5 shows the influence of the respective players ratings on the outcome. As expected players with the higher rating will win but around the 45 degree line there are some exceptions. One would assume that there would be more white wins at the same or similar rating but at different levels this seems to change. Between 1000 and 1200 white seems to win more than black but between 1300 and 1700 it appears that black wins more than white and above 1700 it appears to be even with the majority of draws occurring above 2000. This highlights that the determinants of the outcomes of games may change at different ratings. Figure 3.6 shows the distribution of the ratings. Both colours have nearly identical distrbutions which makes sense as a player will have to play with both colours. The median of black is 1562 and the median of white is 1567. This distinction is used later to assess whether it is easier to predict lower or higher rated games. This section has provided some key insights of the variables of interest and show that they can have an influence on the outcome.
```{r hist, fig.align='center', fig.cap="Distribution of Ratings\\label{Figure7}", fig.ext='png', fig.height=3, fig.width=4, warning=FALSE}
rating <- master_df %>% 
    select(white_rating, black_rating) 
 
median <- rating %>% 
    summarise(white = median(white_rating), black = median(black_rating))

rating_hist <-master_df %>% 
    ggplot() +
  geom_density(aes(x = white_rating), color = "gray") +
     geom_density(aes(x = black_rating), color = "black") +
     geom_vline(xintercept = 1567, color = "gray", linetype = "dashed") +
  geom_vline(xintercept = 1562, color = "black", linetype = "dashed") +
  labs(title = "Player Rating", x = "Rating", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.ticks.x = element_blank())

rating_hist
```

# Methodology
I use a gradient boosting model with the winner of the game as the target and the features being both players ratings, their rating difference, the opening used, the number of moves played in the opening, the time controls and the first five moves of each player. I stratified the sampling by the opening to ensure that as many openings were included in both the training and test split. This also means that the moves were stratified as the moves correspond to the opening. Gradient boosting employs a sequence of shallow trees that learn from the previous tree and makes improvements. I used the XGBOOST package and due to the target variable not being binary and because I had more than 53 categories for the opening feature which the random forrest package in R could not handle. I used the "multi:softmax" objective function to be able to predict three types of outcomes and the evaluation metric was a multiclass log loss which evaluates the logarithm of the predicted probabilities for each class and aggregates them across all instances and is calculated as follows.

\begin{equation}
\text{{mlogloss}} = -\frac{1}{n} \sum \left[ y \cdot \log(p) + (1-y) \cdot \log(1-p) \right]
\end{equation}

The methods used follow @homl2019. I first trained and tested the model with default parameters before employing a grid search that uses cross validation to determine the optimal value of the learning rate ($\eta$), the minimum loss reduction($\gamma$), the L2 regularization term ($\lambda$) and the number of trees. I then used the results of the grid search to hyper parameter tune the model. I then used the importance of each factor and a confusion matrix to interpret and assess the model. I then subset the data using the median ratings and used same method and steps to assess if it is easier or more difficult to predict stronger or weaker players. 

# Results
This section presents the results of all six models. I present the importance of the features of the models with the default parameters to see how the importance changes after hyperparameter tuning. For the tuned models I also present the sensitivity and specificity calculated through a confusion matrix.

## Full Sample

```{r results='asis'}
library(recipes)
library(xgboost)
master_dfull<- master_df %>% 
    separate(increment_code, into = c("base", "increment"), sep = "\\+") %>% 
    mutate(base = as.numeric(base), increment = as.numeric(increment))


set.seed(555)
split_1 <- initial_split(master_dfull, prop = 0.7, strata = "opening_eco")  # Split the dataset 
train_2 <- training(split_1)  # Training set
test_2 <- testing(split_1)  # Test set
factor_cols <- c("opening_eco", "whitemove1", "whitemove2", "whitemove3", "whitemove4", "whitemove5", "blackmove1", "blackmove2", "blackmove3", "blackmove4", "blackmove5")
train_2[factor_cols] <- lapply(train_2[factor_cols], as.character)
test_2[factor_cols] <- lapply(test_2[factor_cols], as.character)


for (col in factor_cols) {
  unique_vals <- unique(c(train_2[[col]], test_2[[col]]))
  train_2[[col]] <- as.integer(factor(train_2[[col]], levels = unique_vals))
  test_2[[col]] <- as.integer(factor(test_2[[col]], levels = unique_vals))
}


X_train <- cbind(train_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], train_2[factor_cols])
X_test <- cbind(test_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], test_2[factor_cols])


train_labels <- as.factor(train_2$winner)
test_labels <- as.factor(test_2$winner)


params <- list(
  objective = "multi:softmax",  
  eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1,  
  seed = 555  
)


dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)  
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.integer(test_labels) - 1)


xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = 1000,  
  early_stopping_rounds = 10,  
  verbose = 0  
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]  


accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)



importance_gain <- xgb.importance(model = xgb_model)

tab <- options(xtable.comment = FALSE)
tab <- xtable(importance_gain, caption = "Importance of Factors")


```

```{r importfull1, fig.align='center', fig.cap="Importance of Features for Untuned Model: Full Sample\\label{Figure8}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}

import_untuned1 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Untuned Model for Full Sample")
import_untuned1


```
Figure 5.1 shows that, as expected, ratings features are the most important features, combined they account for `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100)`% and are the three most important features of the model. The opening that is played is only the fifth most important factor at only `r round(importance_gain$Importance[5]*100)`%. The results also show that the moves increase in importance with both white and black's first moves being the least important features and every subsequent move becomes more important. Black's moves are also shown to be more important to the model than white's moves with black's fifth move being the fourth most important feature at `r round(importance_gain$Importance[4]*100)`% which is interesting as it means it is more important than the opening. The model has an accuracy of 100% on the training set and just 60% on the testing set so it may be over fitted. I then used hyper parameter tuning to attempt to address this issue. Table 5.1 below presents the most optimal parameters provided by the grid search. The results suggest the learning rate ($\eta$) should be reduced from 0.1 to 0.01. The parameters $\alpha$, $\gamma$ and $\lambda$ do not change and are still zero. The tree depth should be reduced from 6 to 3, which should also help solve the issue of over fitting. These parameters are used to obtain the final model and the results are presented below. 


```{r results='asis'}
# 
# 
# # Convert the predicted class labels to a factor with levels
# class_predictions <- factor(class_predictions, levels = levels(test_labels))
# 
# # Create the confusion matrix
# cm <- confusionMatrix(data = class_predictions, reference = test_labels)
# cm1 <-as.data.frame(cm[["byClass"]]) %>% 
#     select(Sensitivity, Specificity)
# # Print the confusion matrix
# cm_tab <- options(xtable.comment = FALSE)
# cm_tab <- xtable(cm1)
# cm_tab
# 
# 

```

```{r grid1}
# hyper_grid <- expand.grid(
#   eta = 0.01,
#   max_depth = 3,
#   min_child_weight =  3,
#   subsample = 0.5,
#   colsample_bytree = c(0.5, 0.75, 1),
#   gamma = c(0, 1, 10, 100, 1000),
#   lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   rmse = 0,
#   trees = 0
# )
# 
# # Grid search
# for (i in seq_len(nrow(hyper_grid))) {
#   set.seed(555)
#   dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)
#   params <- list(
#     objective = "multi:softmax",
#     eval_metric = "mlogloss",
#     num_class = 3,
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i],
#     gamma = hyper_grid$gamma[i],
#     lambda = hyper_grid$lambda[i],
#     alpha = hyper_grid$alpha[i]
#   )
#   
#   cv_result <- xgb.cv(
#     data = dtrain,
#     params = params,
#     nrounds = 2000,
#     early_stopping_rounds = 50,
#     nfold = 10,
#     verbose = 0
#   )
#   
#   hyper_grid$rmse[i] <- min(cv_result$evaluation_log$test_mlogloss_mean)
#   hyper_grid$trees[i] <- cv_result$best_iteration
# }
# 
# # Filter and sort the results
# filtered_results <- hyper_grid %>%
#   filter(rmse > 0) %>%
#   arrange(rmse)
# 
# # Print the filtered results
# glimpse(filtered_results)
# 
# hg1 <- options(xtable.comment = FALSE)
# hg1 <- xtable(filtered_results, caption = "Hypergrid Full Sample")
# hg1
```


\begin{table}[H]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & $\eta$ & Depth & Weight & Subsample & Colsample& $\gamma$ & $\lambda$ & $\alpha$ & RMSE & Trees \\ 
  \hline
1 & 0.01 & 3.00 & 3.00 & 0.50 & 0.50 & 0.00 & 0.00 & 0.00 & 0.80 & 874.00 \\ 
   \hline
\end{tabular}
\caption{Hypergrid Full Sample} 
\end{table}

```{r Final_fit_fullsample}

params <- list(
    eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1, 
  eta = 0.01,
  max_depth = 3,
  min_child_weight = 3,
  subsample = 0.5,
  colsample_bytree = 0.5
)


xgb.fit.final <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 874,
objective = "multi:softmax",
verbose = 0
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]  


accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)
accuracy <- sum(class_predictions == test_labels) / length(test_labels)

```


```{r importfullsamp2, fig.align='center', fig.cap="Importance of Features for Tuned Model: Full Sample\\label{Figure9}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}
importance_gain <- xgb.importance(model = xgb.fit.final)
import_tuned1 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Tuned Model for Full Sample")
import_tuned1

```


```{r results='asis'}

class_predictions <- factor(class_predictions, levels = levels(test_labels))


cmfull2 <- confusionMatrix(data = class_predictions, reference = test_labels)
cmfull2 <-as.data.frame(cmfull2[["byClass"]]) %>% 
    select(Sensitivity, Specificity)


cmfull2_tab <- options(xtable.comment = FALSE)
cmfull2_tab <- xtable(cmfull2, caption = "Accuracy Full Sample: Tuned Model") 
cmfull2_tab

```
Figure 5.2 and table 5.2 present the results of the tuned model. The tuning of parameters has improved the accuracy on the testing set to `r round(accuracy*100, digits = 2)`% but again the accuracy on the training set is still `r round(accuracy_train*100, digits = 2)`% which means that even after tuning the model still suffers from over fitting. The importance of the rating features increased to `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100, digits = 2)`%. The lower learning rate results in better accuracy but it seems that the increased accuracy is driven by the ratings features rather than moves. The opening now becomes the most important feature after ratings which shows that it can influence the game but its importance decreases to just `r round(importance_gain$Importance[4]*100, digits = 2)`%. The time controls also become relatively more important after tuning but similarly to the opening it percentage of importance also decreases.

Table 5.2 is an extract of the model's confusion matrix where sensitivity is the percentage of white wins accurately predicted and specificity is the percentage of white not winning accurately predicted. The table shows that the model predicts white the best with a sensitivity and specificity of `r round((cmfull2$Sensitivity[3]*100), digits = 2)`% and `r round((cmfull2$Specificity[3]*100), digits = 2)`% respectively. It predicts black almost as well with a sensitivity and specificity of `r round((cmfull2$Sensitivity[1]*100))`% and `r round((cmfull2$Specificity[1]*100), digits = 2)`% respectively, unlike for white the specificity is larger than the sensitivity meaning it predicts black not winning more accurately than black winning. The model struggles the most to predict a draw with a sensitivity of just `r round((cmfull2$Sensitivity[2]*100), digits = 2)`%. The accuracy is still nearly double that of just guessing which would be 33%.

```{r treeplot, fig.align='center', fig.cap="Full Sample Model Tree\\label{Figure10}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}


treesplot <- xgb.plot.multi.trees(
  feature_names = names(master_dfull),
  features_keep = 3,
  model = xgb.fit.final
)


```


## Sub-samble by ELO
The sample was then subset using the medians of ratings calculated in the exploratory data analysis section. I use the same methodology as above for both subsets of the sample.

### Bottom Half
```{r Low}
#Lets now get two more models for above and below the median elo using the same code as before
master_dflow<- master_df %>% 
    separate(increment_code, into = c("base", "increment"), sep = "\\+") %>% 
    mutate(base = as.numeric(base), increment = as.numeric(increment)) %>% 
    filter(white_rating < 1567, black_rating < 1562)


set.seed(555)
split_1 <- initial_split(master_dflow, prop = 0.7, strata = "opening_eco")  # Split the dataset 
train_2 <- training(split_1)  # Training set
test_2 <- testing(split_1)  # Test set
factor_cols <- c("opening_eco", "whitemove1", "whitemove2", "whitemove3", "whitemove4", "whitemove5", "blackmove1", "blackmove2", "blackmove3", "blackmove4", "blackmove5")
train_2[factor_cols] <- lapply(train_2[factor_cols], as.character)
test_2[factor_cols] <- lapply(test_2[factor_cols], as.character)


for (col in factor_cols) {
  unique_vals <- unique(c(train_2[[col]], test_2[[col]]))
  train_2[[col]] <- as.integer(factor(train_2[[col]], levels = unique_vals))
  test_2[[col]] <- as.integer(factor(test_2[[col]], levels = unique_vals))
}

X_train <- cbind(train_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], train_2[factor_cols])
X_test <- cbind(test_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], test_2[factor_cols])


train_labels <- as.factor(train_2$winner)
test_labels <- as.factor(test_2$winner)


params <- list(
  objective = "multi:softmax",  
  eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1,  
  seed = 555 
)


dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)  

dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.integer(test_labels) - 1)


xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = 1000,  
  early_stopping_rounds = 10, 
  verbose = 0  
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]  


accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)


```


```{r results='asis'}
# # Convert the predicted class labels to a factor with levels
# class_predictions <- factor(class_predictions, levels = levels(test_labels))
# 
# # Create the confusion matrix
# cmlow1 <- confusionMatrix(data = class_predictions, reference = test_labels)
# cmlow1 <-as.data.frame(cmlow1[["byClass"]]) %>% 
#     select(Sensitivity, Specificity)
# 
# # Print the confusion matrix
# cmfull2_tab <- options(xtable.comment = FALSE)
# cmfull2_tab <- xtable(cmlow1, caption = "Accuracy Bottom Sample: Untuned Model") 
# cmfull2_tab
```

```{r importancelow1, fig.align='center', fig.cap="Feature Importance Untuned Model: Bottom Half\\label{Figure11}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}
importance_gain <- xgb.importance(model = xgb_model)


import_untuned2 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Untuned Model for Bottom Half")
import_untuned2


```

Figure 5.3 shows the feature importance of the untuned model which has an accuracy of 58% which is 2% lower than the full sample. Again ratings feasures are the most important and combined contribute `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100, digits = 2)`% to the model. The relative importance of openings is much lower than in the full sample as it is only the eighth most important factor now with both white and black's fourth and fifth moves contributing more to the model. The results of the grid search are presented below in table 5.3 and the results suggest the same parameters as for the full sample with the exception of $\lambda$ and $\gamma$ which are now both 1. The results of the hyper parameter tuned model are presented below.

```{r}
# hyper_grid <- expand.grid(
#   eta = 0.01,
#   max_depth = 3,
#   min_child_weight =  3,
#   subsample = 0.5,
#   colsample_bytree = c(0.5, 0.75, 1),
#   gamma = c(0, 1, 10, 100, 1000),
#   lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   rmse = 0,
#   trees = 0
# )
# 
# # Grid search
# for (i in seq_len(nrow(hyper_grid))) {
#   set.seed(555)
#   dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)
#   params <- list(
#     objective = "multi:softmax",
#     eval_metric = "mlogloss",
#     num_class = 3,
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i],
#     gamma = hyper_grid$gamma[i],
#     lambda = hyper_grid$lambda[i],
#     alpha = hyper_grid$alpha[i]
#   )
#   
#   cv_result <- xgb.cv(
#     data = dtrain,
#     params = params,
#     nrounds = 2000,
#     early_stopping_rounds = 50,
#     nfold = 10,
#     verbose = 0
#   )
#   
#   hyper_grid$rmse[i] <- min(cv_result$evaluation_log$test_mlogloss_mean)
#   hyper_grid$trees[i] <- cv_result$best_iteration
# }
# 
# # Filter and sort the results
# filtered_results3 <- hyper_grid %>%
#   filter(rmse > 0) %>%
#   arrange(rmse) %>% 
#     head(15)
# 
# # Print the filtered results
# glimpse(filtered_results3)
# 
# hg2 <- options(xtable.comment = FALSE)
# hg2 <- xtable(filtered_results3, caption = "Hypergrid Bottom Half of the Sample")
# hg2
```

\begin{table}[H]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & $\eta$ & Depth & Weight & Subsample & Colsample& $\gamma$ & $\lambda$ & $\alpha$ & RMSE & Trees \\  
  \hline
1 & 0.01 & 3.00 & 3.00 & 0.50 & 0.50 & 1.00 & 1.00 & 0.00 & 0.80 & 603.00 \\ 

   \hline
\end{tabular}
\caption{Hypergrid Bottom Half} 
\end{table}
```{r LowFinal}

params <- list(
    eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1, 
  eta = 0.01,
  lambda = 1,
  gamma = 1,
  max_depth = 4,
  min_child_weight = 3,
  subsample = 0.5,
  colsample_bytree = 0.5
)



xgb.fit.finallow <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 874,
objective = "multi:softmax",
verbose = 0
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]  


accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)

```


```{r importancelow2, fig.align='center', fig.cap="Feature Importance Tuned Model: Bottom Half\\label{Figure12}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}
importance_gain <- xgb.importance(model = xgb.fit.finallow)
import_tuned2 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Tuned Model for Bottom Half")
import_tuned2

```

```{r results='asis'}


class_predictions <- factor(class_predictions, levels = levels(test_labels))
test_labels <- factor(test_labels, levels = levels(class_predictions))


cmlow2 <- confusionMatrix(data = class_predictions, reference = test_labels)
cmlow2 <- as.data.frame(cmlow2[["byClass"]]) %>% 
  select(Sensitivity, Specificity)


cmlow2_tab <- xtable(cmlow2, caption = "Accuracy Bottom Sample: Tuned Model")
cmlow2_tab

```


The tuned model still performs worse than the full sample even after tuning with an accuracy of only `r round(accuracy*100, digits = 2)`%. This makes sense as weaker players will not stick to rigid openings as they do not know much chess theory. This makes them more unpredictable and therefore it is expected that the model performs worse. The model still seems to be over fitted as the accuracy on the training set is still `r round(accuracy_train*100, digits = 2)`%. 

Figure 5.4 presents the importance of features in the tuned model. The decrease in the learning rate parameter does not increase the importance of the ratings features as much as the tuned model for the full sample as it only increases to `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100, digits = 2)`%. The type of opening used now increases in both relative and absolute importance. It is now the fourth most important feature and its importance increased to `r round(importance_gain$Importance[4]*100, digits = 2)`% from 6%. The decrease in the learning rate helped to identify that even for weaker players the opening used is the most important feature after rating features.

Table 5.4 breaks down the models accuracy and again it best predicts white, then black and still struggles with predicting a draw. All values of sensitivity and specificity have decreased compared to the full sample so it can be concluded isolating the weaker players does not improve the models accuracy.


### Top Half
```{r High}
master_dfhigh<- master_df %>% 
    separate(increment_code, into = c("base", "increment"), sep = "\\+") %>% 
    mutate(base = as.numeric(base), increment = as.numeric(increment)) %>% 
        filter(white_rating > 1567, black_rating > 1562)



set.seed(555)
split_1 <- initial_split(master_dfhigh, prop = 0.7, strata = "opening_eco")  # Split the dataset 
train_2 <- training(split_1)  # Training set
test_2 <- testing(split_1)  # Test set
factor_cols <- c("opening_eco", "whitemove1", "whitemove2", "whitemove3", "whitemove4", "whitemove5", "blackmove1", "blackmove2", "blackmove3", "blackmove4", "blackmove5")
train_2[factor_cols] <- lapply(train_2[factor_cols], as.character)
test_2[factor_cols] <- lapply(test_2[factor_cols], as.character)


for (col in factor_cols) {
  unique_vals <- unique(c(train_2[[col]], test_2[[col]]))
  train_2[[col]] <- as.integer(factor(train_2[[col]], levels = unique_vals))
  test_2[[col]] <- as.integer(factor(test_2[[col]], levels = unique_vals))
}


X_train <- cbind(train_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], train_2[factor_cols])
X_test <- cbind(test_2[, c("white_rating", "black_rating", "opening_ply", "ratingdiff", "base", "increment")], test_2[factor_cols])


train_labels <- as.factor(train_2$winner)
test_labels <- as.factor(test_2$winner)


params <- list(
  objective = "multi:softmax",  
  eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1,  
  seed = 555  
)


dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)  
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.integer(test_labels) - 1)


xgb_model <- xgboost(
  data = dtrain,
  params = params,
  nrounds = 1000, 
  early_stopping_rounds = 10, 
  verbose = 0 
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]   


accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)


```

```{r results='asis'}
# class_predictions <- factor(class_predictions, levels = levels(test_labels))
# cmhigh1 <- confusionMatrix(data = class_predictions, reference = test_labels)
# 
# cmhigh1 <-as.data.frame(cmhigh1[["byClass"]]) %>% 
#     select(Sensitivity, Specificity)
# 
# # Print the confusion matrix
# cmhigh1_tab <- options(xtable.comment = FALSE)
# cmhigh1_tab <- xtable(cmhigh1, caption = "Accuracy Top Sample: UNTuned Model") 
# cmhigh1_tab
```

```{r impoerancehigh1, fig.align='center', fig.cap="Feature Importance Untuned Model: Top Half\\label{Figure13}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}


importance_gain <- xgb.importance(model = xgb_model)


import_untuned3 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Untuned Model for Top Half")
import_untuned3

```

Figure 5.3 shows the feature importance of the untuned model which has an accuracy of `r round(accuracy*100, digits = 2)`% which is 2% lower than the full sample. This is interesting as I would have expected the stronger players to be easier to predict as they play more rigid chess and follow chess principles more stringently. Again ratings features are the most important and combined contribute `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100, digits = 2)`% to the model. The importance of the opening for the stronger players is apparent already as it is the fourth most important factor at `r round(importance_gain$Importance[4]*100, digits = 2)`%. The ordering of the importance of the rest of the does not change compared to both other samples. Table 5.5 shows the results of the grid search and the parameter values are the same as for the bottom sample with the exception of $\lambda$ which is 0, similar to the full sample. The parameters from the grid search are then used to hyper parameter tune the model and the results are presented below.

```{r gridsearch3}
# hyper_grid <- expand.grid(
#   eta = 0.01,
#   max_depth = 3,
#   min_child_weight =  3,
#   subsample = 0.5,
#   colsample_bytree = c(0.5, 0.75, 1),
#   gamma = c(0, 1, 10, 100, 1000),
#   lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
#   rmse = 0,
#   trees = 0
# )
# # Grid search
# for (i in seq_len(nrow(hyper_grid))) {
#   set.seed(555)
#   dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.integer(train_labels) - 1)
#   params <- list(
#     objective = "multi:softmax",
#     eval_metric = "mlogloss",
#     num_class = 3,
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i],
#     gamma = hyper_grid$gamma[i],
#     lambda = hyper_grid$lambda[i],
#     alpha = hyper_grid$alpha[i]
#   )
#   
#   cv_result <- xgb.cv(
#     data = dtrain,
#     params = params,
#     nrounds = 2000,
#     early_stopping_rounds = 50,
#     nfold = 10,
#     verbose = 0
#   )
#   
#   hyper_grid$rmse[i] <- min(cv_result$evaluation_log$test_mlogloss_mean)
#   hyper_grid$trees[i] <- cv_result$best_iteration
# }
# 
# # Filter and sort the results
# filtered_results4 <- hyper_grid %>%
#   filter(rmse > 0) %>%
#   arrange(rmse) %>% 
#     head(15) %>% 
#     head(15)
# 
# # Print the filtered results
# glimpse(filtered_results4)
# 
# hg3 <- options(xtable.comment = FALSE)
# hg3 <- xtable(filtered_results4, caption = "Hypergrid Top Half")
# hg3
```

\begin{table}[H]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & $\eta$ & Depth & Weight & Subsample & Colsample& $\gamma$ & $\lambda$ & $\alpha$ & RMSE & Trees \\ 
  \hline
1 & 0.01 & 3.00 & 3.00 & 0.50 & 0.50 & 1.00 & 0.00 & 0.00 & 0.81 & 676.00 \\ 

   \hline
\end{tabular}
\caption{Hypergrid Top Half} 
\end{table}


```{r HighFinal}

params <- list(
    eval_metric = "mlogloss",  
  num_class = 3,  
  nthread = 1, 
  eta = 0.01,
  max_depth = 3,
  gamma = 1,
  min_child_weight = 3,
  subsample = 0.5,
  colsample_bytree = 0.5
)



xgb.fit.finalhigh <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 676,
objective = "multi:softmax",
verbose = 0
)


predictions <- predict(xgb_model, dtest)


train_predictions <- predict(xgb_model, dtrain)
labels_train <- levels(train_labels)
class_predictions_train <- labels_train[as.integer(train_predictions) + 1]


labels <- levels(test_labels)
class_predictions <- labels[as.integer(predictions) + 1]  



accuracy <- sum(class_predictions == test_labels) / length(test_labels)
accuracy_train <- sum(class_predictions_train == train_labels) / length(train_labels)
    
```


```{r importancehigh2, fig.align='center', fig.cap="Feature Importance Tuned Model: Top Half\\label{Figure14}", fig.ext='png', fig.height=5, fig.width=6, warning=FALSE}
importance_gain <- xgb.importance(model = xgb.fit.finalhigh)
import_tuned3 <-xgboost::xgb.ggplot.importance(importance_gain, "Importance of Factors: Tuned Model for Top Half")
import_tuned3
```

```{r results='asis'}
class_predictions <- factor(class_predictions, levels = levels(test_labels))
cmhigh2 <- confusionMatrix(data = class_predictions, reference = test_labels)

cmhigh2 <-as.data.frame(cmhigh2[["byClass"]]) %>% 
    select(Sensitivity, Specificity)


cmhigh2_tab <- options(xtable.comment = FALSE)
cmhigh2_tab <- xtable(cmhigh2, caption = "Accuracy Top Sample: Tuned Model") 
cmhigh2_tab
```

The hyper parameter tuning does not increase the overall accuracy by much and it is still `r round(accuracy*100, digits = 2)`% on the test set and `r round(accuracy_train*100, digits = 2)`% for the training set. This is puzzling as not only is there no improvement, it also worse than for the bottom half sample. Over fitting is still an issue, the hyper parameter tuning does not seem to solve. Figure 5.6 shows the importance of the features and the importance of the ratings features increased to `r round((importance_gain %>% select(Importance) %>%  head(3) %>% summarise(rating_total = sum(Importance)))*100, digits = 2)`% which is much larger than for the bottom sample. Simalarly the importance of the opening for stronger players is only `r round(importance_gain$Importance[4]*100, digits = 2)`%. 

Table 5.6 presents the results from the confusion matrix. It shows that the model predicts black winning and losing more accurately than for the weaker players but predicts white winning and losing less accurately than for the weaker players. Its accuracy and predicting the game to be a draw also decreases. While my assumption going in was that it would be easier to predict the outcome for stronger players was incorrect it highlights how complex the game of chess is and that stronger players seem to rely less on the openings than weaker players.

# Conclusion
Chess is a complicated game and trying to predict the outcome based just on the opening does not work. My results suffer from over fitting which is a large limitation of the investigation. The results show that the type of opening used is a determinant of the outcome but is minuscule compared to the ratings features. The importance of the opening also seems to decrease for players that are stronger, indicating that they rely less on specific opening strategies and may adapt their gameplay based on the opponent's moves and overall game dynamics. Subsetting the data based on player ratings, either by considering the bottom or top half, resulted in models that were less accurate compared to the analysis on the full sample. This suggests that having a diverse range of player ratings in the dataset improves the model's accuracy, likely due to the increased variability and complexity introduced by players of different skill levels.

In conclusion, while the opening moves do play a role in determining the outcome of a chess game, they are overshadowed by the ratings features of the players. To build a more accurate prediction model, it is important to consider a wide range of player ratings and incorporate other relevant features beyond just the opening moves. Chess remains a challenging game to predict, highlighting the intricate nature of the game and the need for comprehensive models that encompass various aspects of player skill and strategy.

\newpage
# References

